Message-ID: <514923A2.60506@lsst.org>
Date: Tue, 19 Mar 2013 21:49:06 -0500
From: Mario Juric <mjuric@lsst.org>
Organization: Large Synoptic Survey Telescope Inc.
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:17.0) Gecko/20130216 Thunderbird/17.0.3
MIME-Version: 1.0
To: Andrew Becker <acbecker@gmail.com>
Subject: Re: Level 1 data products writeup (draft #1)
References: <5147F046.8090004@lsst.org> <5148B561.3020602@gmail.com>
In-Reply-To: <5148B561.3020602@gmail.com>
X-Enigmail-Version: 1.5.1
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: 7bit

On 3/19/13 13:58 , Andrew Becker wrote:
> Hi Mario - My comments are:
> 

Thanks! See below for further questions/clarification.

PS: can CC my reply to everyone else? I'd like to keep everyone involved
included in the discussion to avoid repetition.

>  * I would remove the "if at all" from the coupling of up-to-date
> database to data release database.  People will truly want/need this, so
> we might as well do it.
> 

My intent with that remark was to point out that with this model, the DR
database doesn't even have to exist and we could still do Level 1. This
may be the case during the first few months of commissioning and ops.

Once we have DR coverage, we'll definitely do associations and send them
out.

>  * DiaSource table will not necessarily have sky, as the difference
> images have no sky. 
> 

Good point. Maybe this is better described as 'local background'? It's
basically what you'd use to compute the detection SNR, but given the
noise will be convolved it's not clear how useful or not this is.

Thoughts?

>  * SNR = of detection, or of (Psf) measurement?
>  

Detection. The idea is to give the end-user a value to filter on to
control the number of (truly unavoidable noise-generated) false positives.

How is the SNR of the PSF measurement defined? I thought that these two
were the same. Or is it the difference of approximate detection PSF vs.
the PSF used for the measurement?

>  * remove trailWidth.  I don't see why we don't forward model a line
> convolved withthe Psf.  Should write a paper on that...
> 

This *is* the forward model of a line, with an additional degree of
freedom that the line can have finite width. My thought was that this
may be a cheap way to detect a few more cosmics or other line-like
artifacts (I admit I haven't really thought it through; you may be right
that setting width=0 is sufficient).

>  * There is value in extendednessof DiaSources (in particular, for
> recognizing comets)
> 

How is extendedness (== 1 - SExtractor's stellaricity?) computed? Is
this something that we can derive from the second moments?

>  * Shouldn't H,G beper-filterfor SSObject?  No point in taking our
> observations to V-band...
> 

Good point. This would also let one construct color indices that are
phase-angle independent.

Btw, here's a useful high-level overview of the (H, G) magnitude system:
http://www.britastro.org/asteroids/dymock4.pdf

>  * Define the process of arriving at the "limited number" of DiaSources
> that have precovery forced phot.  E.g. they are new.
> 

Agreed that we need to clarify this. "Limited" in this context will mean
"as much as we can support". I'm hoping that Jacek and K-T tell me we
could do forced photometry going back, say, ~30 days, for *all* newly
detected objects (I believe we're keeping a ~30 day cache of calexps on
disk).

Forced photometry going further back than the cache allows us to go
could then be done over a longer period of time (~days to weeks, instead
of hours).

>  * I presume that we will make new templates during Data Release
> productions.  This will yield an entirely new set of difference images. 

Yes, and (in general) DIASources an DIAObjects.

> Also, do we plan on doing amp- or sensor-level diffim in nightly
> processing?   If amp-level, specifify that the data release diffims will
> be different in that they will likely be sensor-level.
> 

This is still TBD, but I believe we're moving to a model where we'll
have to do per-chip (and beyond) processing even with nightly data.
E.g., you're finding that to derive the spatial PSF model that works at
the edges you need the information for adjacent sensors.

Thanks for all the comments. I'll incorporate them as soon as I'm back
in Tucson,
- M.

> 
> Andy
> 
> 
> 
> On 03/18/13 21:57, Mario Juric wrote:
>> Dear DM crowd (and Zeljko),
>> 	Attached to this e-mail you'll find the first draft of a writeup on
>> Level 1 processing and data products. I'm asking for your initial round
>> of comments, preferably by this Friday. Over the weekend, I'll
>> incorporate any commends I receive and would aim to send it for round #2
>> of comments to the Science Council and the Science Collaboration chairs.
>> By the end of the month I (optimistically) hope to have ready part #2 of
>> that document, that deals with Level 2. We're aiming for an external
>> review in late May.
>>
>> 	The document is intended to capture at a detailed yet readable and
>> understandable level how we plan to generate L1 products, when, what
>> they will consist of, and how they will be distributed. Our DB team will
>> use it to derive the details of database and schema design, the science
>> teams will use it to verify that LSST plans capture science
>> requirements, and it will be used to populate the UML model to enable
>> requirements tracking and software design consistency checks. It will
>> also be the primary document we will give to our data products
>> reviewers, and it's not inconceivable that we may turn it into a
>> companion to the "Overview Paper".
>>
>> 	Please comment on technical feasibility, scientific completeness,
>> clarity, style, etc. At this point, the document is a statement on what
>> I would *like* to be done, not necessarily what we actually *can* do
>> given the resources. There are still many TBDs and TODOs in the text --
>> your thoughts on those would be appreciated. There are some new ideas
>> (e.g., "trailed source model"). There are also some things that were
>> left out wrt. the previous baseline (http://ls.st/wxv) -- please point
>> out those that I should have kept.
>>
>> PS: In case you need it, TeX sources are available at http://ls.st/r7z
>> PPS: The document has *not* been spell-checked. Ignore any such mistakes.
>> PPPS: I owe many of you replies to e-mails; I'll do that tomorrow while
>> on the way to FNAL for DES review. I've been largely ignoring e-mail
>> today in order to finish this document.
>>
>> Cheers,
> 


-- 
Mario Juric,
Data Mgmt. Project Scientist, Large Synoptic Survey Telescope
Web   :  http://www.cfa.harvard.edu/~mjuric/
Phone :  +1 617 744 9003       PGP: ~mjuric/crypto/public.key
