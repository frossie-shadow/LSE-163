%
% LSST Data Products Definition Document
%
% Maintained by Mario Juric <mjuric@lsst.org>
%
\documentclass[12pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{comment}

\excludecomment{changelog}

\newcommand\x         {\hbox{$\times$}}
\newcommand\othername {\hbox{$\dots$}}
\def\eq#1{\begin{equation} #1 \end{equation}}
\def\eqarray#1{\begin{eqnarray} #1 \end{eqnarray}}
\def\eqarraylet#1{\begin{mathletters}\begin{eqnarray} #1 
                  \end{eqnarray}\end{mathletters}}
\def\mic              {\hbox{$\mu{\rm m}$}}
\def\about            {\hbox{$\sim$}}
\def\Mo               {\hbox{$M_{\odot}$}}
\def\Lo               {\hbox{$L_{\odot}$}}
\def\comm#1           {{\tt (COMMENT: #1)}}
\def\kms   {\hbox{km s$^{-1}$}}

\usepackage[usenames]{color} 
\newcommand{\G}[1]{{\color{red} #1}}
\newcommand{\B}[1]{{#1}}
\newcommand{\R}[1]{{\color{red}}}
\newcommand{\code}[1]{\texttt{#1}}

\usepackage{xspace}
\newcommand{\DIASource}{\code{DIASource}\xspace}
\newcommand{\DIASources}{\code{DIASources}\xspace}
\newcommand{\DIAObject}{\code{DIAObject}\xspace}
\newcommand{\DIAObjects}{\code{DIAObjects}\xspace}
\newcommand{\DB}{{Level 1 database}\xspace}
\newcommand{\DR}{{Level 2 database}\xspace}
\newcommand{\Object}{\code{Object}\xspace}
\newcommand{\Objects}{\code{Objects}\xspace}
\newcommand{\Source}{\code{Source}\xspace}
\newcommand{\Sources}{\code{Sources}\xspace}
\newcommand{\ForcedSource}{\code{ForcedSource}\xspace}
\newcommand{\ForcedSources}{\code{ForcedSources}\xspace}
\newcommand{\CoaddSource}{\code{CoaddSource}\xspace}
\newcommand{\CoaddSources}{\code{CoaddSources}\xspace}
\newcommand{\SSObject}{\code{SSObject}\xspace}
\newcommand{\SSObjects}{\code{SSObjects}\xspace}
\newcommand{\VOEvent}{\code{VOEvent}\xspace}
\newcommand{\VOEvents}{\code{VOEvents}\xspace}

\title{Large Synoptic Survey Telescope \\
Data Products Definition Document \\
(*** DRAFT ***)}
\author{
    Mario Juri\'c \textless\href{mailto:mjuric@lsst.org}{mjuric@lsst.org}\textgreater \vspace{1ex} \\
    {\em with input from} \vspace{1ex} \\
    T. Axelrod, A.C. Becker, J. Becla,  G.P. Dubois-Felsmann, \\
    M. Freemon, \v{Z}. Ivezi\'c, J. Kantor, K-T Lim, R. H. Lupton, \\
    D. Shaw, M. Strauss, {\em and} J.A. Tyson \vspace{1.2ex} \\
    {\em for the LSST Project}
}

\begin{document}
\maketitle

\begin{abstract}
This document describes the plans for contents of Level 1 and 2 LSST data products, and the rationale behind various choices that were made. This is an {\bf internal draft} and a work in progress. {\bf It should not be circulated widely until this notice is removed.}
\end{abstract}

\tableofcontents

\section{Introduction}

% Note: paragraph lifted from Zeljko's overview paper
LSST will be a large, wide-field ground-based optical telescope system
designed to obtain multiple images covering the sky that is visible from Cerro Pach\'{o}n in Northern Chile. The current baseline design, with an 8.4m (6.7m effective) primary mirror, a 9.6 deg$^2$ field of view, and a 3.2 Gigapixel camera, will allow about 10,000 square degrees of sky to be covered using pairs  of 15-second exposures \R{in two photometric bands} \B{twice per night} every three nights on average, with typical 5$\sigma$ depth for point sources of $r\sim24.5$ (AB). The system is designed to yield high image quality as well as superb astrometric  and photometric accuracy. The \B{total} survey area will include 30,000 deg$^2$ with $\delta<+34.5^\circ$, and will be imaged multiple times in six bands, $ugrizy$, covering the wavelength range 320--1050 nm. The project is scheduled to  begin the regular survey operations at the start of next decade. About 90\% of the observing time will be devoted to a deep-wide-fast survey mode which will \B{uniformly} observe a 18,000 deg$^2$ region about 1000 times (summed over all six bands) during the anticipated 10 years of operations, and yield a coadded map to $r\sim27.5$. These data will result in databases including 10 billion galaxies and a similar number of stars, and will serve the majority of the primary science programs. The remaining 10\% of the observing time will be allocated to special projects such as a Very Deep and Fast time domain survey.

The LSST will be operated in fully automated survey mode. The images acquired by the LSST Camera will be processed by LSST Data Management software to a) detect and characterize imaged astrophysical sources and b) detect and characterize changes in time in LSST-observed universe. The results of that processing will be reduced images, catalogs of detected objects and the measurements of their properties, and prompt alerts to ``events'' -- changes in astrophysical scenery discovered by differencing incoming images against older, deeper, images of the sky in the same direction ({\em templates}, see \S \ref{sec:templates}).

\vspace{1em}

The {\em broad}, {\em high-level}, requirements for LSST Data Products are given by the LSST Science Requirements Document. This document lays out the {\em specifics} of what the data products will comprise of, how those data will be generated, and when. It serves to inform the flow-down from the {\em LSST Science Requirements Document} and the {\em LSST Observatory System Specifications}, to the {\em LSST Data Management System Requirements} document, the UML model, and the database schema.

\subsection{Level 1 and 2 Data Products}

LSST Data Management will perform two, somewhat overlapping in scientific intent, types of image analyses:

\begin{enumerate}
\item Analysis of difference images, with the goal of detecting and characterizing astrophysical phenomena revealed by their time-dependent nature. The detection of supernovae superimposed on bright extended galaxies is an example of this analysis. The processing is done on a nightly or daily basis and produces {\bf Level 1} data products. They include the difference images, the sources detected in difference images (\DIASources), astrophysical objects\footnote{The LSST has adopted the nomenclature by which single-epoch detections of astrophysical {\em objects} are called {\em sources}. This nomenclature is not universal: some surveys call {\em detections} what we call {\em sources}, and use the term {\em sources} for what we call {\em objects}.} these are associated to (\DIAObjects), and Solar System objects (\SSObjects\footnote{\SSObjects used to be called call ``Moving Objects'' in previous versions of the Data Products baseline. The name is potentially confusing as high-proper motion stars are moving objects as well. A more accurate distinction is the one between objects inside and outside of the Solar System.}). These are added to the {\bf \DB} and made available in real time. Notifications (``alerts'') about new \DIASources will be issued using community-accepted standards within 60 seconds of observation.
\item Analysis of direct images, with the goal of detecting and characterizing astrophysical objects. Detection of faint galaxies on deep co-adds and their subsequent characterization is an example of this analysis. The results are {\bf Level 2} data products. These products, released annually\footnote{Except for the first two data releases, which will be created six months apart.}, will include the single-epoch images, deep co-adds, catalogs of \Objects (detections on deep co-adds) and \Sources\footnote{When written in bold monospace type (i.e., {\tt \textbackslash{}tt}), \Objects and \Sources refer to objects and sources detected and measured as a part of Level 2 processing.} (measurements on individual direct images), as well as fully reprocessed Level 1 data products (see \S \ref{sec:l1dbreproc}). In contrast to the \DB, which is updated in real-time, the \DR{}s are static and will not change after release.
\end{enumerate}
 
The two types of analyses have different requirements on timeliness. Changes in flux or position of objects may need to be immediately followed up, lest interesting information be lost. Thus the primary results of analysis of difference images -- discovered and characterized \DIASources{} -- generally need to be broadcast as {\em event alerts} within 60 seconds of end of visit\footnote{The LSST takes two (nominally 15 second) exposures per pointing, called {\em snaps}. That pair of exposures is called a {\em visit}.} acquisition. The analysis of science (direct) images is less time sensitive, and will be done as a part of annual data release process.


% In both cases, the software analyzes the image data to detect {\em sources}, groupings of pixels with values inconsistent with being noise at some preset level (eg., a typical threshold is $S/N = 5$). If the detection is performed on science images, we call the resulting sources {\em Sources}\footnote{Note the capitalization}. If the source has been detected on a difference image, we call it a {\em DIASource}\footnote{for {\em Difference Image Analysis Source}}.

% Once detected, the sources can be associated to {\em Objects}, and be characterized in various ways (eg., by PSF flux measurement, model fitting, shape measurement, etc.).

\section{Level 1 Data Products}

\subsection{Overview}

Level 1 data products are a result of difference image analysis (DIA; \S \ref{sec:dia}). They include the sources detected in difference images (\DIASources), astrophysical objects that these are associated to (\DIAObjects), identified Solar System objects\footnote{The LSST SRD considers Solar System object orbit catalog to be a Level 2 data product (LSSTSRD, Sec 3.5). Nevertheless, to successfully differentiate between apparitions of known Solar System objects and other types \DIASources we consider it functionally a part of Level 1.} (\SSObject), and related, broadly defined, metadata (including eg., cut-outs\footnote{Small, $30 \times 30$, sub-images at the position of a detected source. Also known as {\em postage stamps.}}).

\DIASources are sources detected on difference images (those above $S/N=5$ after correlation with the PSF profile). They represent changes in flux with respect to a deep template. Physically, a \DIASource may be an observation of new astrophysical object that was not present at that position in the template image (for example, an asteroid), or an observation of flux change in an existing source (for example, a variable star). Their flux can be negative (eg., if a source present in the template image reduced its brightness, or moved away).

Clusters of \DIASources detected on visits taken at different times are associated with either a \DIAObject or an \SSObject to represent the underlying astrophysical phenomenon. The association can be made in two different ways: by assuming the underlying phenomenon is an object within the Solar System moving on an orbit around the Sun\footnote{We don't plan to fit for motion around other Solar System bodies; eg., identifying new satellites of Jupiter is left to the community.}, or by assuming it to be distant enough to only exhibit small parallactic and proper motion\footnote{Where 'small' is small enough to unambiguously positionally associate together individual apparitions of the object.}. The latter type of association is performed during difference image analysis right after the image has been acquired. The former is done at daytime by the Moving Objects Processing Software (\code{MOPS}), unless the \DIASource is an apparition of an already known \SSObject. In that case, it will be flagged as such during difference image analysis.

At the end of the difference image analysis, we will generate alerts for all newly discovered \DIASources.\footnote{For observations on the ecliptic near the opposition, Solar System objects will dominate the \DIASource counts, and (until they're recognized as such) overwhelm the explosive transient signal. It will therefore be advantageous to quickly identify the majority of Solar System objects early in the survey.}.

\subsection{Level 1 Data Processing}

\subsubsection{Difference Image Analysis}
\label{sec:dia}

The following is a high-level description of steps which will occur during regular difference image analysis:
\begin{enumerate}
\item A visit is acquired and reduced to a single {\em visit image} (cosmic ray rejection, instrumental signature removal\footnote{Eg., subtraction of bias and dark frames, flat fielding, bad pixel/column interpolation, etc.}, combining of snaps}, etc.).
\item The visit image is differenced against the appropriate template and \DIASources are detected.
\item The flux and shape\footnote{The ``shape'' in this context consists of weighted 2$^{\rm nd}$ moments, as well as a fit to a trailed source model.} of the DIASource are measured on the difference image. The visit image is force-photometered at the position of the \DIASource to obtain a measure of the absolute flux. No deblending will be attempted.
\item The \DB (see \S \ref{sec:level1db}) is searched for a \DIAObject or \SSObject with which to positionally associate the observed \DIASource\footnote{The association algorithm will guarantee that a \DIASource is associated with not more than one \DIAObject or \SSObject. The algorithm will take into account the parallax and proper or Keplerian motions, as well as the errors in estimated positions of \DIAObject, \SSObject, and \DIASource to find the maximally likely match. Multiple \DIASources in the same visit will not be matched to the same \DIAObject.}. If no match is found, a new \DIAObject is created and the observed \DIASource is associated to it.
\item If the \DIASource has been associated with an \SSObject (a known Solar System object), it will be flagged as such and an alert will be issued. Further processing will occur in daytime (see section \ref{sec:ssProcessing}).
\item Otherwise, the associated \DIAObject measurements will be updated with new data. All affected columns will be recomputed, including proper motions, centroids, light curves, etc.
\item The \DR\footnote{\DR is a database resulting from annual data release processing.} is searched for one or more \Objects positionally close to the \DIAObject, out to some maximum radius\footnote{Eg., a few arcseconds.}. The IDs of these \Objects are recorded in the \DIAObject record and provided in the event alert.
\item An alert is issued that includes: the name of the \DB, the timestamp of when this database has been queried to issue this alert, the \DIASource ID, the \DIAObject ID\footnote{We guarantee that a receiver will always be able to regenerate the alert contents at any later date using the included timestamps and metadata (IDs and database names).}, name of the \DR and the IDs of nearby \Objects, and the associated science content (centroid, fluxes, low-order lightcurve moments, periods, etc.), {\em including the full light curves}. See Section \ref{sec:voEventContents} for a more complete enumeration.
\item For all \DIAObjects overlapping the field of view, to which a \DIASource from this visit has not been associated, forced photometry will be performed (point source photometry only). Those measurements will be stored as appropriately flagged \DIASources\footnote{For the purposes of this document, we're treating the \DIASources generated by precovery measurements to be the same as \DIASources detected in difference images (but flagged appropriately). In the logical schema, these may be divided into two separate tables.}.  No alerts will be issued for these \DIASources.
\item Within 24 hours of discovery, {\em precovery} PSF forced photometry will be performed on any difference image overlapping the position of new \DIAObjects taken within the past 30 days, and added to the database. Alerts will not be issued with precovery photometry information.
\end{enumerate}

In addition to the processing described above, a smaller sample of sources detected on difference images {\em below} the nominal $S/N=5$ threshold will be measured and stored, in order to enable monitoring of difference image analysis quality.

Also, the system will have the ability to measure and alert on a limited\footnote{It will be sized for no less than $\sim 10\%$ of average \DIASource per visit rate.} number of sources detected below the nominal threshold for which additional criteria are satisfied. For example, a $S/N = 3$ source detection near a gravitational keyhole may be highly significant in assessing the danger posed by a potentially hazadous asteroid. The project will define the initial set of criteria by the start of Operations.

\subsubsection{Solar System Object Processing}
\label{sec:ssProcessing}

The following will occur during regular Solar System object processing (in daytime\footnote{Note that there {\em is no guarantee on when daytime Solar System processing must finish}, just that, averaged over some reasonable timescale (eg., a month), a night's worth of observing is processed within 24 hours. Nights rich in moving objects may take longer to process, while nights with less will finish more quickly. In other words, the requirement is on {\em throughput}, not latency.}, after a night of observing):
\begin{enumerate}
\item The orbits/physical properties of \SSObjects that were re-observed on the previous night are recomputed. Updated data are entered to the \SSObjects table.
\item All \DIASources detected on the previous night, that have not been matched with high probability to a known \Object, \SSObject, or an artifact, are analyzed for potential pairs, forming {\em tracklets}.
\item The collection of tracklets collected over the past 30 days is searched for subsets forming {\em tracks} consistent with being on the same Keplerian orbit around the Sun.
\item For those that are, an orbit is fitted and a new \SSObject table entry created. \DIASource records are updated to point to the new \SSObject record. \DIAObjects ``orphaned'' by this unlinking are deleted.\footnote{Some \DIAObjects may only be left with forced photometry measurements at their location (since all \DIAObjects are force-photometered on previous and subsequent visits);  these will be kept but flagged as such.}.
\item Precovery linking is attempted for all \SSObjects whose orbits were updated in this process. Where successful, \SSObjects (orbits) are updated as needed.
\end{enumerate}

\subsection{The \DB}
\label{sec:level1db}

The described alert processing design presupposes the existence of a \DB that contains the objects and sources detected on difference images. At the very least\footnote{It will also contain exposure and visit metadata, MOPS-specific tables, etc. These are either standard/uncontroversial, or implementation-dependent, irrelevant for science, and therefore not discussed here.}, this database will have tables of \DIASources, \DIAObjects, and \SSObjects, populated in the course of difference image and Solar System object processing\footnote{The latter is also colloquially known as {\em DayMOPS}.}. As these get updated and added to, their updated contents becomes visible (queryable) immediately\footnote{No later than the moment of issuance of any event alert that may refer to it.}.

Note that {\em this database is only loosely coupled to the \DR}. All of the coupling is through providing positional matches between the \DIAObjects entries in the \DB and the \Objects in the \DR database. There is no direct \DIASource-to-\Object match. The adopted data model emphasizes that {\em having a \DIASource be positionally coincident with an \Object does not imply it is physically related to it}. Absent other information, the least presumptuous data model relationship is one of {\em positional association}, not {\em physical identity}.

This may seem odd at first: for example, in a simple case of a variable star, matching individual \DIASources to \Objects is exactly what an astronomer would want. That approach, however, fails in the following scenarios:
\begin{itemize}
\item {\em A supernova in a galaxy.} The matched object in the \Object table will be the galaxy, which is a distinct astrophysical object. We want to keep the information related to the supernova (eg., colors, the light curve) separate from those measurements for the galaxy.
\item {\em An asteroid occulting a star.} If associated with the star on first apparition, the association would need to be dissolved when the source is recognized as an asteroid (perhaps even as early as a day later).
\item {\em A supernova on top of a pair of blended galaxies.} It is not clear in general to which galaxy this \DIASource would belong. That in itself is a research question.
\end{itemize}

\vspace{1ex}
\DIASource-to-\Object matches can still be emulated via a three-step link (\DIASource-\DIAObject-\Object). For ease of use, views or pre-built table with these will be offered to end-users.

% There are three ``core'' tables in the \DB: the \DIASource table, with information about detected and/or measured \DIASources, \DIAObject table, with summary information about \DIAObjects derived from the associated \DIASources, and the \SSObject table (short for {\bf Solar System Object}\footnote{This is what we used to call a ``Moving Object''. This name is potentially confusing, as high-proper motion stars are moving objects as well. A more accurate distinction is the one between objects in an out of the Solar System.}) holding derived orbits and associated Solar System Object-specific information.

\vspace{2em}

In the sections to follow, we present the {\em conceptual schemas} for the most important \DB tables. These convey {\em what} data will be recorded in each table, rather than the details of {\em how}. For example, columns whose type is an array (eg., \texttt{radec}) may be expanded to one table column per element of the array (eg., \texttt{ra}, \texttt{decl}) once this schema is translated to SQL. Secondly, the tables to be presented are normalizes (i.e., contain no redundant information). For example, since the band of observation can be found by joining a \DIASource table to the table with exposure metadata, there's no column for 'band' in the \DIASource table. In the as-built database, the views presented to the users will be appropriately denormalized for ease of use.

\subsubsection{\DIASource Table}

This is a table of sources detected at $SNR \geq 5$ on difference images (\DIASources). On average, we expect $\sim 2000$ \DIASources per visit ($\sim 2{\rm M}$ per night; 20,000 per deg$^2$ per hour).

Some $SNR \geq 5$ sources will not be caused by observed astrophysical phenomena, but by artifacts (bad columns, diffraction spikes, etc.). The difference image analysis software will attempt to identify and flag these as such.

Unless noted otherwise, all \DIASource quantities (fluxes, centroids, etc.) are measured on the difference image.

\begin{center}
\begin{longtable}{p{3cm}p{2cm}p{2cm}p{5cm}}
\caption[\DIASource Table]{\DIASource Table
} \\

\hline \multicolumn{1}{c}{\bf Name} & \multicolumn{1}{c}{\bf Type} & \multicolumn{1}{c}{\bf Unit} & \multicolumn{1}{c}{\bf Description} \\ \hline
\endhead

\hline \multicolumn{4}{r}{{\em Continued on next page}} \\
\endfoot

\hline\hline
\endlastfoot

diaSourceId & uint128 & ~ & Unique source identifier \\ 

ccdVisitId & uint64 & ~ & Id. of CCD and visit where this source was measured \\ 

diaObjectId & uint128 & ~ & Id. of the \DIAObject this source was associated with, if any. \\ 

ssObjectId & uint64 & ~ & Id. of the \SSObject this source has been linked to, if any. \\ 

midPointTai & double & time & Time of mid-exposure for this DIASource. \\ 

radec & double[2] & degrees & $(\alpha, \delta)$\footnote{The astrometric reference frame will be chosen closer to start of operations.} \\ 

radecCov & float[3] & various & \texttt{radec} covariance matrix \\ 

xy & float[2] & pixels & Column and row of the centroid. \\ 

xyCov & float[3] & various & Centroid covariance matrix \\ 

SNR & float & ~ & The signal-to-noise ratio at which this source was detected in the difference image.\footnote{This is not necessarily the same as psFlux/psFluxSigma, as the flux measurement algorithm may be more accurate than the detection algorithm.} \\

psFlux & float & nmgy\footnote{A ``maggie'', as introduced by SDSS, is a linear measure of flux; one maggie has an AB magnitude of 0. ``nmgy'' is short for a nanomaggie. Flux of $0.063$~nmgy corresponds to a $24.5^{\rm th}$ magnitude star. See \S \ref{sec:fluxes} for details.} & Calibrated flux for point source model. Note this actually measures the flux {\em difference} between the template and the visit image. \\ 

psFluxSigma & float & nmgy & Estimated uncertainty of \texttt{psFlux}. \\

psLnL & float & ~ & Natural $log$ likelihood of the observed data given the point source model. \\ 

trailFlux & float & nmgy & Calibrated flux for a trailed source model\footnote{A {\em Trailed Source Model} attempts to fit a (PSF-convolved) model of a point source that was trailed by a certain amount in some direction (taking into account the two-snap nature of the visit, which may lead to a dip in flux around the mid-point of the trail). Roughly, it's a fit to a PSF-convolved line. The primary use case is to characterize fast-moving Solar System objects.}$^,$\footnote{This model does not fit for the {\em direction} of motion; to recover it, we would need to fit the model to separately to individual snaps of a visit. This adds to system complexity, and is not clearly justified by increased MOPS performance given the added information.}. Note this actually measures the flux {\em difference} between the template and the visit image. \\ 

trailLength & float & arcsec & Maximum likelihood fit of trail length\footnote{Note that we'll likely measure trailRow and trailCol, and transform to trailLength/trailAngle (or trailRa/trailDec) for storage in the database. A stretch goal is to retain both.}$^,$\footnote{TBD: Do we need a separate trailCentroid? It's unlikely that we do, but one may wish to prove it.}. \\ 

trailAngle & float & degrees & Maximum likelihood fit of the angle between the meridian through the centroid and the trail direction (bearing). \\ 

trailLnL & float & ~ & Natural $log$ likelihood of the observed data given the trailed source model. \\ 

trailCov & float[6] & various & Covariance matrix of trailed source model parameters. \\ 

fpFlux & float & nmgy & Calibrated flux for point source model measured on the visit image centered at the centroid measured on the difference image (forced photometry flux) \\ 

fpFluxSigma & float & nmgy & Estimated uncertainty of \texttt{fpFlux}. \\ 

fpSky & float & nmgy/asec$^{2}$ & Estimated sky background at the position (centroid) of the object. \\ 

fpSkySigma & float & nmgy/asec$^{2}$ & Estimated uncertainty of \texttt{fpSky}. \\ 

%grayExtinction & float & nmgy & Applied photometric extinction correction (gray component) \\ 

%nonGrayExtinction & float & nmgy & Applied photometric extinction correction (color-dependent component) \\ 

E1 & float & ~ & Adaptive $e_1$ shape measure of the source as measured on the difference image\footnote{See Bernstein \& Jarvis (2002) for detailed discussion of all adaptive-moment related quantities, or \url{http://ls.st/5f4} for a brief summary.}. \\

E2 & float & ~ & Adaptive $e_2$ shape measure. \\

E1E2cov & float[3] & ~ & {\tt E1}, {\tt E2} covariance matrix. \\

mSum & float & ~ & Sum of second adaptive moments. \\

mSumSigma & float & ~ & Uncertainty in {\tt mSum} \\

extendedness & float & ~ & A measure of extendedness, computed using a combination of available moments and model fluxes or from a likelihood ratio of point/trailed source models (exact algorithm TBD). $extendedness=1$ implies a high degree of confidence that the source is extended. $extendedness=0$ implies a high degree of confidence that the source is point-like. \\

flags & bit[64] & bit & Flags \\ \hline
\end{longtable}
\end{center}

\begin{changelog}
Notes about changes with respect to the previous baseline:
\begin{itemize}
\item I removed the \texttt{astromRefr*} columns. These will depend on the SED (color) of the object, and the color won't be know when the object is discovered. It may be better to provide a UDF to compute the refraction given a \DIAObject record.
\item Removed "small galaxy" model fits. We don't plan to do galaxy model fits on difference images.
\item Removed "canonical small galaxy" model fits. See above.
\item Removed galExtinction: this should be a UDF using extinction maps
\item I removed the aperture correction column.
\item gray/nonGray extinction columns removed. May be implemented as an UDF.
\item TODO: See what other fields SDSS has. Also see what fields PanSTARRS has. Collect input from SCs.
\end{itemize}
\end{changelog}

\subsubsection{\DIAObject Table}

\begin{center}
\begin{longtable}{p{3cm}p{2cm}p{2cm}p{5cm}}
\caption[\DIAObject Table]{\DIAObject Table} \\

\hline \multicolumn{1}{c}{\bf Name} & \multicolumn{1}{c}{\bf Type} & \multicolumn{1}{c}{\bf Unit} & \multicolumn{1}{c}{\bf Description} \\ \hline
\endhead

\hline \multicolumn{4}{r}{{\em Continued on next page}} \\
\endfoot

\hline\hline
\endlastfoot

diaObjectId & uint128 & ~ & Unique identifier \\ 

radec & double[2] & degrees & $(\alpha, \delta)$ position of the object at time \texttt{radecTai} \\ 

radecCov & float[3] & various & \texttt{radec} covariance matrix \\ 

radecTai & double & time & Time at which the object was at a position \texttt{radec}. \\ 

pm & float[2] & mas/yr & Proper motion vector\footnote{High proper-motion or parallax objects will appear as ``dipoles" in difference images. Great care will have to be taken not to misidentify these as subtraction artifacts.} \\ 

parallax & float & mas & Parallax \\ 

pmParallaxCov & float[6] & various & Proper motion - parallax covariances. \\ 

psFlux & float[ugrizy] & nmgy & Weighted mean point-source model magnitude. \\ 

psFluxErr & float[ugrizy] & nmgy & Standard error of {\tt psFlux}  \\ 

psFluxSigma & float[ugrizy] & nmgy & Standard deviation of the distribution of {\tt psFlux}. \\ 

fpFlux & float[ugruzy] & nmgy & Weighted mean forced photometry flux.\\

fpFluxErr & float[ugrizy] & nmgy & Standard error of {\tt fpFlux} \\ 

fpFluxSigma & float[ugrizy] & nmgy & Standard deviation of the distribution of {\tt fpFlux}. \\ 

lsPeriod  & float[ugrizy] & day & Period (the coordinate of the highest peak in Lomb-Scargle periodogram) \\

lsSigma  & float[ugrizy] & day & Width of the peak at \texttt{lsPeriod}. \\

lsPower   & float[ugrizy] & ~ & Power associated with \texttt{lsPeriod} peak. \\

lcChar   & float[$6\times{}M$] & ~ & Light-curve characterization summary statistics (eg., 2nd moments, etc.). The exact contents, and an appropriate value of M, are to be determined in consultation with time-domain experts. \\

nearbyObj   & uint128[3] & ~ & Closest \Objects in \DR. \\

nearbyObjDist   & float[3] & arcsec & Distances to \texttt{nearbyObj}. \\

flags & bit[64] & bit & Flags \\ \hline

\end{longtable}
\end{center}

\subsubsection{\SSObject Table}

\begin{center}
\begin{longtable}{p{3cm}p{2cm}p{2cm}p{5cm}}
\caption[\SSObject Table]{\SSObject Table} \\

\hline \multicolumn{1}{c}{\bf Name} & \multicolumn{1}{c}{\bf Type} & \multicolumn{1}{c}{\bf Unit} & \multicolumn{1}{c}{\bf Description} \\ \hline
\endhead

\hline \multicolumn{4}{r}{{\em Continued on next page}} \\
\endfoot

\hline\hline
\endlastfoot

ssObjectId & uint64 & ~ & Unique identifier \\ 

oe & double[7] & various & Osculating orbital elements at epoch ($q$, $e$, $i$, $\Omega$, $\omega$, $M_0$, epoch) \\

oeCov & double[21] & various & Covariance matrix for \texttt{oe} \\

arc & float & days & Arc of observation. \\

orbFitLnL & float & ~ & Natural log of the likelohood of the orbital elements fit. \\

nOrbFit & int16 & ~ & Number of observations used in the fit. \\

MOID & float[2] & AU & Minimum orbit intersection distances\footnote{\url{http://www2.lowell.edu/users/elgb/moid.html}} \\

moidLon & double[2] & degrees & MOID longitudes. \\

H & float[6] & mag & Mean absolute magnitude, per band. \\

G & float[6] & mag & Fitted slope parameter, per band\footnote{The slope parameter for the large majority of asteroids will not be well constrained until later in the survey. We may decide not to fit for it at all over the first few DRs, and add it later in Operations. Alternatively, we may fit it with a strong prior.} \\

hErr & float[6] & mag & Uncertainty in estimate of H \\

gErr & float[6] & mag & Uncertainty in estimate of G \\

flags & bit[64] & bit & Flags \\ \hline

\end{longtable}
\end{center}

The LSST database will provide functions to compute the phase (Sun-Asteroid-Earth) angle $\alpha$ for every observation, as well as the reduced ($H(\alpha)$) and absolute ($H$) asteroid magnitudes.

\begin{changelog}
Notes about changes with respect to the previous baseline:
\begin{itemize}
\item Though many columns have been removed, we should maintain roughly the equivalent extra columns in the sizing model as some may re-appear internally (eg., MOPS-specific columns). This is true in general for all tables.
\item Removed all asteroid shape-related columns; determining these is outside of the scope of the Project.
\item Removed taxonomy related columns; determining these is outside of the scope of the Project.
\item Removed \texttt{albedo} -- I don't believe albedo can be determined solely from LSST data. More likely, we will need to assume a particular value. If this value is not universal, this column will need to be put back in.
\item Removed \texttt{xMag, xMagErr, xAmplitude, xPeriod} columns as they were not clearly defined (eg., w/o phase correction, do they make sense?). These can be recovered by querying the \DIASource table for magnitudes. Deriving phase-corrected light curves is left as a Level 3 task.
\item Removed a number of other MOPS-specific columns. These are algorithm-specific and should not be a part of the baseline, outward-facing, schema\footnote{Because we may change the algorithm and they may disappear; the scientists should not be relying on them being there.}. They will need to be documented and added back into the physical schema, for sizing purposes.
\end{itemize}
\end{changelog}

\subsubsection{Estimator and Naming Conventions}

We employ a convention where estimates of standard errors have the suffix {\tt Err}, while the estimates of inherent widths of distribution (or functions in general) have the suffix {\tt Sigma}\footnote{Given $N$ measurements, standard errors scale as $N^{-1/2}$, while widths remain constant.}. The latter are defined as the square roots of the second moment about the quoted value of the quantity at hand.

Unless noted otherwise, maximum likelihood values are be quoted for all fitted parameters (measurements). Together with covariances, these let the end-user apply whatever prior they deem appropriate when computing posteriors\footnote{There's a tacit assumption that a Gaussian is a reasonably good description of the likelihood surface around the ML peak.}.

For fluxes, we recognize that a substantial fraction of astronomers will just want the posteriors marginalized over all other parameters, trusting the LSST experts to select an appropriate prior\footnote{It's likely that most cases will require just the expectation value alone.}. For example, this is nearly always the case when constructing color-color or color-magnitude diagrams. We will support these use cases by providing additional pre-computed columns, taking care to name them appropriately so as to minimize accidental incorrect usage. For example, a column named \texttt{gFlux} may be the expectation value of the g-band flux, while \texttt{gFluxML} may be the maximum likelihood value.

\subsubsection{Fluxes and Magnitudes}
\label{sec:fluxes}

Because flux measurements on difference images are performed against a template, the measured flux of a source on the difference image can be negative. The flux can also go negative for faint sources in the presence of noise. Negative fluxes cannot be stored as (Pogson) magnitudes; $\log$ of a negative number is undefined. We've therefore decided to store fluxes rather than magnitudes, in database tables\footnote{This is a good idea in general. Eg. given multi-epoch observations, one should always be averaging fluxes, rather than magnitudes.}.

We quote fluxes in units of ``maggie''. A maggie, as introduced by SDSS, is a linear measure of flux. An object with flux of one maggie (integrated over the bandpass) has an AB magnitude of zero:
\begin{equation}
    m_{AB} = -2.5 \log_{10}(f/{\rm maggie})}
\end{equation}

We chose to use maggies (as opposed to Jansky) to allow the user to differentiate between two separate sources of calibration error: the error in relative (internal) calibration of the survey, and the error in absolute calibration that depends on the knowledge of absolute flux of photometric standards.

\vspace{1em}
We realize that the large majority of users will want to work with magnitudes. For convenience, we plan to provide columns with (Pogson) magnitudes\footnote{These will most likely be implemented as ``virtual" or ``computed" columns}, where values with negative flux will evaluate to \code{NULL}. Similarly, we will provide columns with flux expressed in Jy (and its error estimates).

\subsubsection{Precovery Measurements}

When a new \DIASource is detected, it's useful to perform (PSF) forced photometry at the location of the new source on images taken prior to discovery. These are colloquially know as {\em precovery measurements}\footnote{When Solar System objects are concerned, precovery has a slightly different meaning: predicting the position of a newly discovered \SSObject on previously acquired visits, and associating with it \DIASources consistent with its predicted position.}. Performing precovery in real time over all previously acquired visits is too I/O intensive to be feasible. We therefore plan the following:
\begin{enumerate}
\item For all newly discovered objects, perform precovery PSF forced photometry on visits taken over the previous 30 days\footnote{We will be maintaining a cache of $30$ days of processed images to support this feature.}.
\item Make available a ``precovery service'' to request precovery for a limited number of \DIASources across all previous visits, and make it available within 24 hours of the request. Web interface and machine-accessible APIs will be provided.
\end{enumerate}

The former should satisfy the most common use cases (eg., SNe), while the latter will provide an opportunity for more extensive timely precovery of targets of special interest.

\subsubsection{Annual Reprocessings}
\label{sec:l1dbreproc}

In what we've described so far, the \DB is continually being added to as new images are taken and \DIASources identified. Every time a new \DIASource is associated to an existing \DIAObject, the \DIAObject record is updated to incorporate new information brought in by the \DIASource. Once discovered and measured, the \DIASources would never be re-discovered and re-measured at the pixel level.

This is not optimal. Newer versions of LSST pipelines will improve detection and measurements on older data. Also, PSF forced photometry should be performed on the position of the \DIAObject on all pre-discovery images. This argues for periodic {\em reprocessing} of the Level 1 data set.

\vspace{1em}

We plan to reprocess all image differencing-derived data (the \DB), at the same time as we perform the annual Level 2 data release productions. This will include all images taken since the start of observation, to the time when the DR production begins. The images will be reprocessed using a single version of the image differencing and measurement software, resulting in a consistent data set.

As reprocessing is expected to take approximately $\sim 9$ months, more imaging will be acquired in the meantime. These data will be reprocessed as well, and added to the new \DB generated by the data release processing. The reprocessed database will thus ``catch up" with the \DB currently in use, possibly in a few increments. Once it does, the existing \DB will be replaced with the new one, and all future alerts will refer to the reprocessed \DB. Alerts for new sources ``discovered" during data release processing and/or the catch-up process will {\em not} be issued.

\vspace{1em}
Note that \DB reprocessing and switch will have {\em significant} side-effects on downstream users. For example, all \DIASource and \DIAObject IDs will change in general. Some \DIASources and \DIAObjects will disappear (eg., if they're image subtraction artifacts that the improved software was now able to recognize as such). New ones may appear. The \DIASource/\DIAObject/\Objects associations will change as well.

While the annual database switches will undoubtedly cause technical inconvenience (eg., a \DIASource detected at some position and associated to one \DIAObject ID on day $T-1$, will now be associated to a different \DIAObject ID on day $T+0$), the resulting database will be a more accurate description of the astrophysics that the survey is seeing (eg., the association on day $T+0$ is the correct one; the associations on $T-1$ and previous days were actually made to an artifact that skewed the \DIAObject summary of measurements).

To ease the transition, third parties (event brokers) may choose to provide positional-crossmatching to older versions of the \DB. A set of best practices will be developed to minimize the disruptions caused by the switches (eg., when writing event-broker queries, filter on position, not on \DIAObject ID, if possible, etc.). A \DB distribution service, allowing for bulk downloads of the reprocessed \DB, will need to be established to support the brokers who will use it locally to perform more advanced brokering\footnote{A ``bulk-download" database distribution service will be provided for the \DR{}s as well, to enable end-users to establish and run local mirrors (partial or full).}.

Older versions of the \DB will be archived following the same rules as for the \DR{}s. DR1, the most recent DR, and the penultimate data release will be kept on disk and loaded into the database. Others will be archived to tape and available as bulk downloads.

\subsubsection{Repeatability of Queries}

We require that queries executed at a known point in time against some version of the \DB be repeatable at a later date. The exact implementation of this requirement is under consideration by the DM database team.

One possibility may be to make the key tables (nearly) append-only, with each row having two timestamps -- createdTai and deletedTai, so that queries may be limited through a \code{WHERE} clause:
%
\begin{quote}
\texttt{SELECT * FROM DIASource WHERE 'YYYY-MM-DD-HH-mm-SS' BETWEEN createdTAI and deletedTAI}
\end{quote}
%
or, more generally:
%
\begin{quote}
\code{SELECT * FROM DIASource WHERE ``data is valid as of YYYY-MM-DD"}
\end{quote}

A perhaps less error-prone alternative, if technically feasible, may be to provide multiple virtual databases that the user would access as:
%
\begin{quote}
\texttt{CONNECT lsst-dr5-yyyy-mm-dd} \\
\texttt{SELECT * FROM DIASource}
\end{quote}
%
The latter method would probably be limited to nightly granularity, unless there's a mechanism to create virtual databases/views on-demand.

\subsubsection{Uniqueness of IDs across database versions}

To reduce the likelihood for confusion, all \Source, \Object, \DIASource, and \DIAObject IDs shall be unique across database versions. For example, DR4 and DR5 reprocessings will share no identical IDs. 

Note, however, that exposure and visit IDs will remain the same across releases.

\subsection{Level 1 Image Products}

\subsubsection{Visit Images}

Raw and processed visit images will be made available for download no later than 300 seconds from the end of visit acquisition.

The images will remain accessible with low-latency (seconds from request to start of download) for at least 30 days, with slower access afterwards (minutes to hours).

\subsubsection{Difference Images}
\label{sec:diffims}

Complete difference images will be made available for download no later than 300 seconds from the end of visit acquisition.

The images will remain accessible with low-latency (seconds from request to start of download) for at least 30 days, with slower access afterwards (minutes to hours).

\subsubsection{Image Differencing Templates}
\label{sec:templates}

Templates for difference image analysis will be created by co-adding 6-months to a year long groups of visits. The co-addition process will take care to remove any transients or fast moving objects (eg., asteroids) from the templates.

The input images may be further grouped by airmass and/or seeing\footnote{The number and optimal parameters for airmass/seeing bins will be determined in Commissioning.}. Therefore, at DR11, we will be creating 11 groups templates: two for the first year of the survey (DR1 and DR2), and then one using imaging from each subsequent year.

Difference image analysis will use the appropriate template given the time of observation, airmass, and seeing.

\subsection{Alerts to \DIASources}
\label{sec:voEventContents}

\subsubsection{Information Contained in Each Alert}

For each detected \DIASource, LSST will emit an ``Event Alert" within 60 seconds of the end of exposure. These alerts will be issued in \VOEvent format\footnote{Or some other format that is broadly accepted and used by the community at the start of LSST commissioning.}, and should be readable by \VOEvent-compliant clients.

\vspace{1em}
Each alert (a \VOEvent packet) will at least include the following:

\begin{itemize}
\item \DB id (example: DR5-Level1)
\item alertTimestamp (A timestamp that can be used to execute a query against the \DB as it existed when this alert was issued)
\item Science Data:
    \begin{itemize}
    \item The \DIASource record that triggered the alert
    \item The entire \DIAObject (or \SSObject) record
    \item All previous \DIASource records
    \end{itemize}
% \item Flags (isSolarSystemObject, isArtefact, etc.)
\item $30\times 30$ pixel cut-out of the difference image (10 bytes/pixel, FITS MEF)
\item $30\times 30$ pixel cut-out of the template image (10 bytes/pixel, FITS MEF)
\end{itemize}

\subsubsection{Receiving and Filtering the Alerts}
\label{sec:eventbrokers}

Alerts will be transmitted in \VOEvent format, using standard IVOA protocols (eg., VOEvent Transport Protocol; VTP). As a very high rate of alerts is expected, approaching $\sim 2$ million per night, we plan for public VOEvent Event Brokers\footnote{These brokers are envisioned to be operated as a public service by third parties who will have signed MOUs with LSST. An example may be the VAO or its successor.} to be the primary end-points of LSST's VTP streams. End-users will use these brokers to classify and filter events on the stream for those fitting their science goals. End-users will {\em not} be able to subscribe to full, unfiltered, alert streams coming directly from LSST\footnote{This is due to finite network bandwidth available: for example, a 100 end-users subscribing to a $\sim 100$ Mbps stream (the peak full stream data rate at end of the first year of operations) would require 10Gbps WAN connection from the archive center, just to serve the alerts.}.

For the end-users, LSST will provide a basic, limited capacity, alert filtering service. This service will run at the LSST archive center (at NCSA). It will let astronomers create simple filters that limit what alerts are ultimately forwarded to them\footnote{More specifically, to their VTP clients. Typically, a user will use the Science User Interface (the web portal to LSST archive center) to set up the filters, and use their VTP client to receive the filtered \VOEvent stream.}. These {\em user defined filters} will be possible to specify using an SQL-like declarative language, or short snippets of (likely Python) code. For example, here's what a filter may look like:
\begin{verbatim}
    # Keep only never-before-seen events within two
    # effective radii of a galaxy. This is for illustration 
    # only; the exact methods/members/APIs may change.
    
    def filter(alert):
        if len(alert.sources) > 1:
            return False
        nn = alert.diaobject.nearest_neighbors[0]
        if not nn.flags.GALAXY:
            return False
        return nn.dist < 2. * nn.Re
\end{verbatim}

We emphasize that this LSST-provided capability will be limited, and is {\em not} intended to satisfy the wide variety of use cases that a full-fledged public Event Broker could. For example, we do not plan to provide any classification (eg., ``is the light curve consistent with an RR Lyra?", or ``a Type Ia SN?"). No information beyond what is contained in the \VOEvent packet will be available to user-defined filters (eg., cross-matches with other catalogs). The complexity and run time of user defined filters will be limited by available resources. Execution latency will not be guaranteed. The number of \VOEvents transmitted to each user per user will be limited as well (eg., at least up to $\sim 20$ per visit per user, dynamically throttled depending on load). Finally, the total number of simultaneous subscribers is likely to be limited -- in case of overwhelming interest, a TAC-like proposal process may be instituted.

\subsection{Open Issues}

What follows is a (non-exhaustive) list of issues, technical and scientific, that are still being discussed and where changes are likely. Input on any of these will be appreciated. These need to be resolved before this document is baselined.

\begin{itemize}
    \item {\em What light-curve metric should we compute and provide with alerts?} We strive to compute general purpose metrics which will facilitate classification. We have not baselined any yet.
    \item {\em Should we measure on individual snaps (or their difference)?} Is there a demonstrable science case requiring immediate followup that would be triggered by the flux change over a $\sim$15 second period? Is it technically feasible?
    \item {\em Should we choose {\tt nearbyObj}s differently?} One proposal is to find the brightest \Object within $XX$ arcsec (with $XX \sim 10$arcsec), and the total number of \Objects within $XX$ arcsec.
    \item {\em Should the postage stamps provided with the alerts be binned, and by what factor?}
    \item {\em When should we (if ever) stop performing forced photometry on positions of \DIAObjects?} Depending on the rate of false positives, unidentified artifacts, or unrecognized Solar System objects, the number of forced measurements may dramatically grow over time.
    \item {\em Can we, should we, and how will we measure proper motions on difference images?} This is a non-trivial task (need to distinguish between dipoles that are artifacts, and those due to proper motions), without a clear science driver (since high proper motion stars will be discoverable using Level 2 catalogs).
    \item {\em Is \DB required to be relational?}. A no-SQL solution may be more appropriate given the followup-driven use cases. Even if it is relational, the Level 1 database will {\em not} be sized or architected to perform well on large or complex queries (eg. complex joins, full table scans, etc.).
    \item {\em Can users query the \DB for all \DIASources next to an \Object?} Is this technically feasible?
    \item {\em Do we have to, and can we, use 128 bit integers for IDs?}. If 64 bit integers are provably sufficient, they will take up less space and be better supported (technologically).

\end{itemize}

\clearpage

\section{Level 2 Data Products}

\subsection{Overview}

Level 2 data products result from direct image\footnote{As opposed to {\em difference image}, in Level 1.} analysis. They're designed to enable {\em static sky} science (eg., studies of galaxy evolution, or weak lensing), and time-domain science that is not time sensitive (eg. statistical investigations of variability). They include images products (reduced single-epoch exposures, called {\em calibrated exposures}, and co-adds), and catalog products (tables of objects, sources, and related metadata).

\vspace{1em}

Similarly to Level 1 catalogs of \DIAObjects and \DIASources, \Objects in the Level 2 catalog represent the astrophysical phenomena (stars, galaxies, quasars, etc.), while \Sources represent their single-epoch observations. \Sources are independently detected and measured in single epoch exposures and recorded in the \Source table. 

The master list of \Objects is generated by associating and deblending the list of single-epoch source detections and the lists of sources detected on special-purpose built {\em deep-coadds}. The co-adds used for \Object detection are designed to be deeper and have better effective seeing than the median visit. At least one co-add per band will be built, a multi-color coadd, and possibly a series of shorter period (eg. yearly) co-adds\footnote{The short-period co-adds are necessary to avoid missing faint objects showing long-term variability. The short-period co-adds will not be preserved. We will provide a facility to regenerate small subsections (up to few deg$^2$) on-demand.}. The flux limit in deep co-adds will be significantly fainter than on single visits, and detected sources will be easier to deblend.

The deblender will be run simultaneously on the catalog of peaks\footnote{The source detection algorithm we plan to employ finds regions of connected pixels above the nominal $S/N$ threshold in the {\em PSF-likelihood image} of the visit. These regions are called {\em footprints}. Each footprint may have one or more {\em peaks}, and it is these peaks that the deblender will use to infer the number and positions of objects blended in each footprint.} detected in the deep co-adds, the \DIAObject catalog from the \DB, and one or more external catalogs.  It will use the knowledge of peak positions, bands, time, time variability (from Level 1 and the single-epoch \Source detections), inferred motion, Galactic longitude and latitude, and other available information to produce a master list of deblended \Objects. Metadata on why and how a particular \Object was deblended will be kept.

The properties of \Objects, including their exact positions, motions, parallaxes, and shapes, will be characterized by MultiFit-type algorithms\footnote{``MultiFit algorithms'' are those that fit a PSF-convolved model to all multi-epoch observations of an object. This is in contrast to measurement techniques where multi-epoch images are co-added first, and the properties are measured from the co-added pixels.}.

Finally, to enable studies of variability the fluxes of all \Objects will be measured on individual epochs while keeping their shape parameters and deblending resolutions constant. This process is know as {\em forced photometry}, and the flux measurements will be stored in the \ForcedSource table.

\subsection{Level 2 Data Processing}
\label{sec:level2dp}

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.5]{Level_2_Processing_Flowchart}
    \caption{Level 2 Data Processing Overview\label{fig:level2dp}}    
\end{figure}

Figure~\ref{fig:level2dp} presents a high-level view of the Level 2 data processing workflow\footnote{Some LSST documents refer to {\em Data Release Processing}, which includes both Level 1 reprocessing (see \S~\ref{sec:l1dbreproc}), and the Level 2 processing described here}. Logically\footnote{The actual implementation may parallelize these steps as much as possible.}, the processing begins with single-frame (visit) image reduction and source measurement, followed by global astrometric and photometric calibration, co-add creation, detection on co-adds, association and deblending, object characterization, and ends with forced photometry measurements.

The following is a high-level description of steps which will occur during regular Level 2 data processing:
\begin{enumerate}
    \item {\em Single Frame Processing}: Raw exposures are reduced to {\em calibrated visit exposures}, and \Sources are independently detected, deblended, and measured on all visits. Their measurements (instrumental fluxes and shapes) are stored in the \Source table.
    \item {\em Relative calibration}: The survey is internally calibrated, both photometrically and astrometrically. Relative zero point and astrometric corrections are computed for every visit. Sufficient data is kept to reconstruct $\phi_b(\lambda)$ at every position in the focal plane at the time of each visit, as required by \S~3.3.4 of the SRD.
    \item {\em Coadd creation}: Deep per-band co-adds are created in $ugrizy$ bands, as well as deeper, multi-color, co-adds\footnote{We'll denote the ``band" of the multi-color co-add as 'M'.}. They will be optimized for a reasonable combination of depth (i.e., no PSF matching) and resolution (i.e., only above-average seeing visits may be used to construct them). Transient sources (including Solar System objects, explosive transients, etc), will be rejected from the co-adds. See \S~\ref{sec:coadds} for details.
    \item {\em Co-add source detection and characterization}. Sources will be detected on all co-adds generated in the previous step. The source detection algorithm will detect regions of connected of pixels, known as {\em footprints}, above the nominal $S/N$ threshold in the {\em PSF-likelihood image} of the visit. Each footprint may have one or more {\em peaks}, and the collection of these peaks (and their membership in the footprints) are the output of this stage. This information will be stored in a catalog of \CoaddSources\footnote{The exact contents of this catalog will be implementation specific and will not be described here.}.
    \item {\em Association and deblending}. The next stage in the pipeline, which we will for simplicity just call {\em the deblender}, synthesizes a list of unique objects. In doing so it will consider the catalog of \Sources, the the catalog of \CoaddSources, \DIAObjects and \DIASources detected on difference images, and objects from external catalogs.
    
    The deblender will make use of all information available at this stage, including the knowledge of peak positions, bands, time, time variability (from Level 1), Galactic longitude and latitude, etc. The output of this stage is a list of (uncharacterized) \Objects\footnote{Depending on the exact implementation of the deblender, this stage may also attach significant metadata (eg, deblended footprints and pixel-weight maps) to each deblended \Object record.}.
    \item {\em Multi-epoch object characterization}. A set of predefined model fits and measurements will be performed on each of the \Objects identified in the previous step, taking all available multi-epoch data into account. Model fits will be performed using {\em MultiFit}-type algorithms. Rather than co-adding a set of images and measuring object characteristics on the co-add, MultiFit simultaneously fits PSF-convolved models to the objects multiple observations. This reduces systematic errors, improves the overall $S/N$, and allows for fitting of time-dependent quantities degenerate with shape on the co-adds (for example, the proper motion). The models we plan to fit will {\em not} allow for flux variability (see the next item).
    \item {\em Forced Photometry}. Source fluxes are measured at every visit, with the position, motion, shape, and the deblending parameters characterized in the previous step kept fixed. This process of {\em forced photometry}, results in characterization of the light-curve for each object in the survey. The fluxes will be stored in the \ForcedSource table.
\end{enumerate}

\subsubsection{Object Characterization Measures}

Properties of detected objects will be measured as a part of the object characterization step described in the previous section and stored in the \Object table. These measurements are designed to enable LSST ``static sky" science. This section discusses at a high level which properties will be measured and how those measurements will be performed. For a detailed list of quantities being fit/measured, see the table in \S~\ref{sec:objectTable}.

All measurements discussed in this section deal with properties of {\em objects}, and will be performed on multi-epoch co-adds, or by simultaneously fitting to all epochs. Measurements of sources in individual visits, independent of all others, are described in \S~\ref{sec:sourceMeas}.

\vspace{1em}

To enable science cases depending on observations of non-variable objects in the LSST-observed sky, plan to measure the following:
%
\begin{itemize}
    \item {\em Point source model fit}. The observed object is modeled as a point source with finite proper motion and parallax and constant flux (a-priori different in each band). This model is a good description for stars and other unresolved sources. Its 11 parameters will be simultaneously constrained using information from all available observations in all bands\footnote{The fitting procedure will account for differential chromatic refraction.}.
    \item {\em Bulge-disk model fit}. The object is modeled as a sum of a de Vaucouleurs (Sersic $n=4$) and an exponential (Sersic $n=1$) component. This model is intended to be a reasonable description of galaxies. The object is assumed not to move. The components share the same ellipticity and center. One effective radius is fit for each component (that is, the radius is {\em not} a function of band). The central surface brightness is allowed to vary from band to band. There are a total of 18 free parameters, which will be simultaneously constrained using information from all available epochs and bands. Where there's insufficient data to constrain the likelihood (eg., small, poorly resolved, galaxies, or very few epochs), priors will be adopted to limit the range of its sampling.

   In addition to the maximum likelihood values of fitted parameters and their covariances, a number (currently planned to be $\sim 200$, on average) of independent samples from the likelihood function will be provided. These will enable use-cases sensitive to departures from Gaussian approximation.

    \item {\em Centroids}. Centroids will be computed independently in each band using an algorithm similar to that employed by SDSS. Information from all epochs will be used to derive the estimate. These centroids will be used for adaptive moment, Petrosian, Kron, standard color, and aperture measurements.

    \item {\em Adaptive moments}. Adaptive moments will be computed using information from all epochs, independently for each band. The moments of the PSF realized at the position of the object will be provided as well.

    \item {\em Standard colors}. Colors of the object in ``standard seeing\footnote{Likely the median expected survey seeing.}" will be measured. These colors are guaranteed to be seeing independent,  suitable for estimation of photometric redshifts\footnote{The problem of optimal determination of photometric redshift is the subject of intense research in the community. The approach we're taking here is conservative, following contemporary practices. As new insights develop, we will revisit the issue.}.

    \item {\em Petrosian and Kron fluxes}. Petrosian and Kron radii and fluxes will be measured using elliptical apertures computed from adaptive moments. The apertures will be PSF-corrected and convolved to a canonical circular PSF ({\em ``PSF-homogenized"\footnote{This is an attempt to derive a definition of elliptical apertures that does not depend on seeing. For example, for a large galaxy, the correction to standard seeing will introduce little change to measured ellipticity. Apertures for small galaxies will tend to be circular (due to smearing by the PSF). But in the intermediate regime, this method results in derived apertures that are relatively seeing independent. Note that this is only the case for {\em apertures}; the measured flux will still be seeing dependent and it is up to the user to take this into account.}}). The radii will be computed independently for each band. Fluxes will be computed in each band, by integrating the light within some multiple of {\em the radius measured in the canonical band}\footnote{The shape of the aperture in all bands will be set by the profile of the galaxy in the canonical band alone. This procedure ensures that the color measured by comparing the flux in different bands is measured through a consistent aperture. See \url{http://www.sdss.org/dr7/algorithms/photometry.html} for details.} (most likely the $i$ band). Radii enclosing 50\% and 100\% of light will be provided.
    \item {\em Aperture fluxes}. Aperture fluxes will be computed in a variable number\footnote{The number will depend on the size of the source.} of concentric, logarithmically spaced, PSF-homogenized, elliptical apertures.

\end{itemize}

\subsubsection{Source Characterization}
\label{sec:sourceMeas}

Sources will be detected on visits as well as the co-adds. Sources detected on the co-adds will primarily serve as inputs to the construction of the master object list as described in \S~\ref{sec:level2dp}, and are not intended to directly support any of LSST science cases.
\\

The following source properties are planned to be measured:
%
\begin{itemize}
    \item {\em Static point source model fit}. The source is modeled as a static point source. There are a total of 3 free parameters ($\alpha$, $\delta$, flux). This model is a good description of stars and other unresolved sources.

    \item {\em Centroids}. Centroids will be computed using an algorithm similar to that employed by SDSS. These centroids will be used for adaptive moment and aperture magnitude measurements.

    \item {\em Adaptive moments}. Adaptive moments will be computed. The moments of the PSF realized at the position of the object will be provided as well.
    
%    \item {\em Petrosian and Kron fluxes}. Petrosian and Kron radii and fluxes will be measured using elliptical apertures computed from adaptive moments. The apertures will be PSF-corrected and convolved to a canonical circular PSF. Fluxes will be computed in each band, by integrating the light within some multiple of {\em the radius measured in the same band}\footnote{Note that this is different}. Radii enclosing 50\% and 100\% of light will be provided.

    \item {\em Aperture fluxes}. Aperture fluxes will be computed in a variable number\footnote{The number will depend on the size of the source.} of concentric, logarithmically spaced, PSF-homogenized, elliptical apertures.

%    \item {\em Bulge-disk model fit}. The object is modeled as a sum of a de Vaucouleurs (Sersic $n=4$) and an exponential (Sersic $n=1$) component. This model is intended to be a reasonable description of galaxies. The object is assumed not to move. The components share the same ellipticity and center. There are a total of 8 free parameters ($\alpha$, $\delta$, $e_1$, $e_2$, $I_{0B}$, $I_{0D}$, $R_{eB}$, $R_{eD}$). Where there's insufficient data to constrain the likelihood (eg., small, poorly resolved, galaxies, or very few epochs), priors will be adopted. Only maximum likelihood values and the covariance matrix will be stored.

\end{itemize}

Note that we do {\em not} plan to fit extended source Bulge+Disk models to individual \Sources, nor measure per-visit Petrosian or Kron fluxes. These are object properties that are not expected to vary in time\footnote{Objects that {\em} do change shape with time would, obviously, be of particular interest. Aperture fluxes provided in the \Source table should suffice to detect these. Further per-visit shape characterization can be performed as a Level 3 task.}, and will be better characterized by MultiFit (the \Object table).

\subsubsection{Forced Photometry}
\label{sec:forcedPhotL2}

% The majority of \Objects in the \Object table, derived by associating the sources in individual epochs and deep co-adds, will be below the single-visit signal-to-noise detection threshold.

% Object characterization with $MultiFit$ assumes that 

{\em Forced Photometry} is the measurement of flux in individual visits, given a fixed position, shape, and the deblending parameters of an object. It enables the study of time variability of an object's flux, irrespective of whether the flux in any given individual visit is above or below the detection threshold.

Forced photometry will be performed on all visits, for all \Objects. The measured fluxes will be stored in the \ForcedSources table. Due to space constraints, we only plan to measure the PSF flux.

\subsection{The Level 2 Catalogs}

This section presents the contents of key Level 2 catalog tables. As was the case for Level 1 (see \S~\ref{sec:level1db}), here we present the {\em conceptual schemas} for the most important Level 2 tables.

These convey {\em what} data will be recorded in each table, rather than the details of {\em how}. For example, columns whose type is an array (eg., \texttt{radec}) may be expanded to one table column per element of the array (eg., \texttt{ra}, \texttt{decl}) once this schema is translated to SQL. Secondly, the tables to be presented are normalized (i.e., contain no redundant information). For example, since the band of observation can be found by joining a \Source table to the table with exposure metadata, there's no column for 'band' in the \Source table. In the as-built database, the views presented to the users will be appropriately denormalized for ease of use.

\subsubsection{The \Object Table}
\label{sec:objectTable}

\begin{center}
\begin{longtable}{p{3cm}p{2cm}p{2cm}p{5cm}}
\caption[\DIASource Table]{Level 2 Catalog \Object Table
} \\

\hline \multicolumn{1}{c}{\bf Name} & \multicolumn{1}{c}{\bf Type} & \multicolumn{1}{c}{\bf Unit} & \multicolumn{1}{c}{\bf Description} \\ \hline
\endhead

\hline \multicolumn{4}{r}{{\em Continued on next page}} \\
\endfoot

\hline\hline
\endlastfoot

objectId & uint128 & ~ & Unique object identifier \\ 

psRadecTai & double & time & Point source model: Time at which the object was at position {\tt radec}. \\

psRadec & double[2] & degrees & Point source model: $(\alpha, \delta)$ position of the object at time {\tt radecTai}. \\

psPm & float[2] & mas/yr & Point source model: Proper motion vector.\\ 

psParallax & float & mas & Point source model: Parallax. \\ 

psFlux & float[ugrizy] & nmgy & Point source model fluxes\footnote{Point source model assumes that fluxes are constant in each band. If the object is variable, {\tt psFlux} will effectively be some estimate of the average flux.}.\\ 

psCov & float[66] & various & Point-source model covariance matrix\footnote{Not all elements of the covariance matrix will be stored with same precision. While the variances will be stored as 32 bit floats ($\sim$ seven significant digits), the covariances may be stored to $\sim$ three significant digits ($\sim 1$\% ).}. \\ 

psLnL & float & ~ & Natural $log$ likelihood of the observed data given the point source model. \\ 


bdRadec & double[2] & degrees & B+D model\footnote{Though we refer to this model as ``Bulge plus Disk", we caution the reader that the decomposition, while physically motivated, should not be taken too literally.}: $(\alpha, \delta)$ position of the object at time {\tt radecTai}, in each band. \\

bdEllip & float[2] & ~ & B+D model: Ellipticity $(e_1, e_2)$ of the object. \\

bdFluxB & float[ugrizy] & nmgy/as$^2$ & B+D model: Total flux of the de Vaucouleurs component. \\

bdFluxD & float[ugrizy] & nmgy/as$^2$ & B+D model: Total flux of the exponential component. \\

bdReB & float & arcsec & B+D model: Effective radius of the de Vaucouleurs profile component. \\

bdReD & float & arcsec & B+D model: Effective radius of the exponential profile component. \\

bdCov & float[171] & various & B+D model covariance matrix\footnote{See {\tt psCov} for notes on precision of variances/covariances.}. \\ 

bdLnL & float & ~ & Natural $log$ likelihood of the observed data given the bulge+disk model. \\ 

bdSamples & float[18][200] & ~ & Independent samples of bulge+disk likelihood surface. All sampled quantities will be stored with at least $\sim 3$ significant digits of precision. The number of samples will vary from object to object, depending on how well the object's likelihood function is approximated by a Gaussian.\\

extColor & float[5] & mag & Color of the object measured in ``standard seeing". While the exact algorithm is yet to be determined, this color is guaranteed to be seeing-independent and suitable for photo-Z determinations.\\

extColorSigma & float[5] & mag & Uncertainty of {\tt extColor}. \\

radec & double[6][2] & arcsec & Position of the object (centroid), computed independently in each band. The centroid will be computed using an algorithm similar to that employed by SDSS.\\

radecSigma & double[6][2] & arcsec & Uncertainty of {\tt radec}. \\


E1 & float[ugrizy] & ~ & Adaptive $e_1$ shape measure. See Bernstein \& Jarvis (2002) for detailed discussion of all adaptive-moment related quantities\footnote{Or \url{http://ls.st/5f4} for a brief summary.}. \\

E2 & float[ugrizy] & ~ & Adaptive $e_2$ shape measure. \\

E1E2cov & float[ugrizy][3] & ~ & {\tt E1}, {\tt E2} covariance matrix. \\

mSum & float[ugrizy] & ~ & Sum of second adaptive moments. \\

mSumSigma & float[ugrizy] & ~ & Uncertainty in {\tt mSum} \\

m4 & float[ugrizy] & ~ & Fourth order adaptive moment. \\


petroRad & float[ugrizy] & arcsec & Petrosian radius computed using. \\

petroRadSigma & float[ugrizy] & arcsec & Uncertainty of {\tt petroRad} \\

petroFlux & float[ugrizy] & nmgy & Petrosian flux within a defined multiple of the canonical {\tt petroRad} \\

petroFluxSigma & float[ugrizy] & nmgy & Uncertainty in {\tt petroFlux} \\

petroRad50 & float[ugrizy] & arcsec & Radius containing 50\% of Petrosian flux. \\

petroRad50Sigma & float[ugrizy] & arcsec & Uncertainty of {\tt petroRad50}. \\

petroRad90 & float[ugrizy] & arcsec & Radius containing 90\% of Petrosian flux. \\

petroRad90Sigma & float[ugrizy] & arcsec & Uncertainty of {\tt petroRad90}. \\


kronRad & float[ugrizy] & arcsec & Kron radius \\

kronRadSigma & float[ugrizy] & arcsec & Uncertainty of {\tt kronRad} \\

kronFlux & float[ugrizy] & nmgy & Kron flux within a defined multiple of the canonical {\tt kronRad} \\

kronFluxSigma & float[ugrizy] & nmgy & Uncertainty in {\tt kronFlux} \\

kronRad50 & float[ugrizy] & arcsec & Radius containing 50\% of Kron flux. \\

kronRad50Sigma & float[ugrizy] & arcsec & Uncertainty of {\tt kronRad50}. \\

kronRad90 & float[ugrizy] & arcsec & Radius containing 90\% of Kron flux. \\

kronRad90Sigma & float[ugrizy] & arcsec & Uncertainty of {\tt kronRad90}. \\


apN & int8 & ~ & Number of elliptical annuli (see below). \\

apMeanSb & float[6][{\tt apN}] & nmgy/asec$^2$ & Mean surface brightness within an annulus\footnote{A database function will be provided to compute the area of each annulus, to enable the computation of aperture flux.}. \\

apMeanSbSigma & float[6][{\tt apN}] & nmgy/asec$^2$ & Standard deviation of {\tt apMeanSb}. \\

flags & bit[128] & bit & Flags \\ \hline
\end{longtable}
\end{center}





\subsubsection{\Source Table}
\label{sec:sourceTable}

\Source measurements are performed independently on individual visits. They're designed to enable astrometric and photometric relative calibration, variability studies of high signal-to-noise objects, and studies of high SNR objects that vary in position and/or shape (eg., comets).

\begin{center}
\begin{longtable}{p{3cm}p{2cm}p{2cm}p{5cm}}
\caption[\Source Table]{Level 2 Catalog \Source Table
} \\

\hline \multicolumn{1}{c}{\bf Name} & \multicolumn{1}{c}{\bf Type} & \multicolumn{1}{c}{\bf Unit} & \multicolumn{1}{c}{\bf Description} \\ \hline
\endhead

\hline \multicolumn{4}{r}{{\em Continued on next page}} \\
\endfoot

\hline\hline
\endlastfoot

sourceId & uint128 & ~ & Unique source identifier\footnote{It would be optimal if the source ID is globally unique across all releases. Whether that's realized will depend on technological and space constraints.} \\ 

ccdVisitId & uint64 & ~ & Id. of CCD and visit where this source was measured \\ 

objectId & uint128 & ~ & Id. of the \Object this source was associated with, if any. \\ 

ssObjectId & uint64 & ~ & Id. of the \SSObject this source has been linked to, if any. \\ 

psFlux & float & nmgy & Calibrated point source model flux.\\ 

psXY & float[2] & pixels & Point source model: $(column, row)$ position of the object on the CCD. \\

psCov & float[6] & various & Point-source model covariance matrix\footnote{Not all elements of the covariance matrix will be stored with same precision. While the variances will be stored as 32 bit floats ($\sim$ seven significant digits), the covariances may be stored to $\sim$ three significant digits ($\sim 1$\% ).}. \\ 

psLnL & float & ~ & Natural $log$ likelihood of the observed data given the point source model. \\ 

psRadec & double[2] & degrees & Point source model: $(\alpha, \delta)$ position of the object, transformed from {\tt psXY} \\

psCov2 & float[6] & various & Point-source model covariance matrix for {\tt psRadec} and {\tt psFlux}.\\ 


xy & float[2] & arcsec & Position of the object (centroid), computed using an algorithm similar to that used by SDSS.\\

xyCov & float[3] & ~ & Covariance matrix for {\tt xy}. \\

radec & double[2] & arcsec & Calibrated ($\alpha$, $\delta$) of the source, transformed from {\tt xy}.\\

radecCov & float[3] & arcsec & Covariance matrix for {\tt radec}. \\


E1 & float & ~ & Adaptive $e_1$ shape measure. \\

E2 & float & ~ & Adaptive $e_2$ shape measure. \\

E1E2cov & float[3] & ~ & {\tt E1}, {\tt E2} covariance matrix. \\

mSum & float & ~ & Sum of second adaptive moments. \\

mSumSigma & float & ~ & Uncertainty in {\tt mSum} \\

m4 & float & ~ & Fourth order adaptive moment. \\



apN & int8 & ~ & Number of elliptical annuli (see below). \\

apMeanSb & float[{\tt apN}] & nmgy & Mean surface brightness within an annulus. \\

apMeanSbSigma & float[{\tt apN}] & nmgy & Standard deviation of {\tt apMeanSb}. \\


flags & bit[128] & bit & Flags \\ \hline
\end{longtable}
\end{center}





\subsubsection{\ForcedSource Table}
\label{sec:forcedSourceTable}


\begin{center}
\begin{longtable}{p{3cm}p{2cm}p{2cm}p{5cm}}
\caption[\ForcedSource Table]{Level 2 Catalog \ForcedSource Table
} \\

\hline \multicolumn{1}{c}{\bf Name} & \multicolumn{1}{c}{\bf Type} & \multicolumn{1}{c}{\bf Unit} & \multicolumn{1}{c}{\bf Description} \\ \hline
\endhead

\hline \multicolumn{4}{r}{{\em Continued on next page}} \\
\endfoot

\hline\hline
\endlastfoot

objectId & uint128 & ~ & Unique object identifier \\ 

ccdVisitId & uint64 & ~ & Id. of CCD and visit where this source was measured \\ 

psXY & float[2] & ~ & Location of the measurement, in pixel coordinates\footnote{The precision of the position has to be no greater than 1/100$^{\rm th}$ of a pixel. For a 4k$\times$4k CCD, {\tt psXY} should require no more than 38 bits of storage (or 5 bytes, rounded up)}. \\

psFlux & float & nmgy & Point source model flux.\\ 

psFluxErr & float & nmgy & Point source model flux error,  stored to 1\% precision.\\

% bdFlux & float & nmgy & Bulge+disk model flux.\\ 

% bdFluxErr & float & nmgy & Bulge+disk model flux error, stored to 1\% precision.\\

flags & bit[8] & bit & Flags \\ \hline
\end{longtable}
\end{center}




\subsection{Level 2 Image Products}

\subsubsection{Visit Images}

Raw, including individual snaps, and processed visit images will be made available for download as FITS files. They will be downloadable both through the human-friendly Science User Interface, as well as machine-friendly APIs.

Sufficient metadata\footnote{Including the hardware and operating system specifications.} and all necessary image processing software will be provided to enable the user to regenerate bitwise identical processed images from raw images.

\subsubsection{Calibration Frames}

All calibration frames (darks, flats, biases, fringe, etc.) will be preserved and made available for download as FITS files.

\subsubsection{Coadded Images}
\label{sec:coadds}

In course of Level 2 processing, three classes of co-adds will be created:
    \begin{itemize}
        \item One set of full depth\footnote{Including all visits from the start of the survey} {\em deep co-adds}. One deep co-add will be created for each of the $ugrizy$ bands, plus a seventh, deeper, multi-color co-add\footnote{In text to follow, we'll denote the ``band" of the multi-color co-add as 'M'.}. These co-adds will be optimized for a reasonable combination of depth (i.e., no PSF matching) and resolution (i.e., only above-average seeing visits may be used to construct them). Transient sources (including Solar System objects, explosive transients, etc), will be removed. Care will be taken to preserve the astrophysical backgrounds\footnote{For example, using ``background matching'' techniques; \url{http://ls.st/l9u}}.
 
        These co-adds will be kept indefinitely and made available to the users. {\em Their primary purpose is to enable studies of diffuse structures}.

               \item Multiple (ugrizyM) sets of yearly co-adds. Each of these sets will be created using only a year's worth of data, otherwise being the same as the deep co-adds described above. These are designed to enable detection of long-term variable or moving\footnote{For example, nearby high proper motion stars.} objects that would be ``washed out" (or rejected) in full-depth co-adds. {\bf We \em do not plan to keep and make these co-adds available}. We will retain and provide sufficient metadata for users to re-create them using Level 3 or other resources.
        \item One (ugrizyM) set of PSF-matched co-adds. These will be used to measure colors of objects at ``standard" seeing. {\bf \em We do not plan to keep and make these co-adds available}. We will retain and provide sufficient metadata for users to re-create them using Level 3 or other resources.
    \end{itemize}

To build the co-adds, we plan to subdivide the sky into 12 overlapping\footnote{We're planning for 3.5 degrees of overlap, roughly accommodating a full LSST focal plane.} {\em tracts}, spanning approximately $75 \times 72$ degrees. The sky will be stereographically projected onto the tracts\footnote{See \url{https://dev.lsstcorp.org/trac/wiki/DM/SAT/SkyMap} for details.}, and be pixelized into (logical) images 2.0 x 1.9 megapixels in size (3.8 terapixels in all). Physically, these large images will be subdivided into smaller (e.g. $2{\rm k} \times 2{\rm k}$), non-overlapping, {\em patches}, though that will be transparent to the users. The users will be able to request arbitrarily chosen regions\footnote{Up to some reasonable upper size limit; i.e., we don't plan to expect to support creation of 2.8 Tpix FITS files.} in each tract, and receive them back as a FITS file.

\vspace{1ex}

We re-iterate that {\bf not all co-adds will be kept and served to the public}\footnote{The co-adds are a major cost driver for storage. LSST Data Management system is currently sized to keep and serve seven co-adds.}, though sufficient metadata will be provided to users to recreate them on their own. Some co-adds may be entirely ``virtual": for example, the PSF-matched co-adds could be implemented as ad-hoc convolutions of postage stamps when the colors are measured.

We {\em \bf will} retain smaller sections of all generated co-adds, to support quality assessment and targeted science. Retained sections may be positioned to cover areas of the sky of special interest such as overlaps with other surveys, nearby galaxies, large clusters, etc.

\subsection{Data Availability}

What's kept and for how long (the data releases on disk vs. tape).

\subsection{Level 2 Open Issues}

\begin{itemize}
    \item {\em Which co-adds to we keep and serve to the public?} Probably non-PSF matched co-adds with CoaddPSF (aka. StackFit PSF). PSF-matched co-adds are another option and may be easier for the users to work with.
\end{itemize}

\section{Deep Drilling Data Products}

Say something about the deep drilling data products. These will involve special nightly stacks, special image differencing procedures, and separate databases. Rougly, since the details of all DDs won't be known for a while (and will change), this section will lay out the constraints on what can and cannot be done for DD processing.

\end{document}
